import asyncio
import json
import os
import logging
import fitz  # PyMuPDF
from aiokafka import AIOKafkaConsumer
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import PGVector

# 1. é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# 2. ç¯å¢ƒå˜é‡åŠ è½½ (ç¡®ä¿ä½ çš„ .env æ˜ å°„åˆ°äº†å®¹å™¨)
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+psycopg2://myuser:password123@db:5432/resume_rag")
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "redpanda:9092")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# 3. åˆå§‹åŒ– AI ç»„ä»¶
# æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æœ‰æ•ˆçš„ OPENAI_API_KEY
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

async def process_resume_rag(resume_id, file_path):
    """çœŸæ­£çš„ RAG å¤„ç†æµç¨‹"""
    try:
        if not os.path.exists(file_path):
            logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            return

        # --- A. æå–æ–‡æœ¬ ---
        logger.info(f"ğŸ“„ æ­£åœ¨è§£æ PDF: {file_path}")
        text = ""
        with fitz.open(file_path) as doc:
            for page in doc:
                text += page.get_text()
        
        if not text.strip():
            logger.warning(f"âš ï¸ PDF å†…å®¹ä¸ºç©º: {resume_id}")
            return

        # --- B. æ–‡æœ¬åˆ‡åˆ† ---
        chunks = text_splitter.split_text(text)
        logger.info(f"âœ‚ï¸ åˆ‡åˆ†å®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # --- C. å‘é‡åŒ–å¹¶å­˜å…¥ PGVector ---
        # æˆ‘ä»¬ä½¿ç”¨ PGVector çš„ from_texts æ–¹æ³•
        # collection_name å¯ä»¥è®©æˆ‘ä»¬æŠŠä¸åŒçš„ç®€å†æ•°æ®éš”ç¦»å¼€ï¼Œæˆ–è€…ç»Ÿä¸€æ”¾åœ¨ä¸€èµ·é€šè¿‡ metadata åŒºåˆ†
        logger.info(f"ğŸ§¬ æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å­˜å…¥æ•°æ®åº“...")
        
        # å°† SQLALCHEMY URL è½¬æ¢ä¸º PGVector å…¼å®¹æ ¼å¼
        # æ³¨æ„ï¼šPGVector é€šå¸¸éœ€è¦ç‰¹æ®Šçš„è¿æ¥å­—ç¬¦ä¸²æ ¼å¼
        connection_str = DATABASE_URL.replace("postgresql+psycopg2://", "postgresql://")
        
        await asyncio.to_thread(
            PGVector.from_texts,
            texts=chunks,
            embedding=embeddings,
            connection_string=connection_str,
            collection_name="resumes_collection",
            metadatas=[{"resume_id": resume_id}] * len(chunks)
        )

        logger.info(f"âœ… RAG ç´¢å¼•æ„å»ºæˆåŠŸ | ID: {resume_id}")

    except Exception as e:
        logger.error(f"ğŸ”¥ å¤„ç†å¤±è´¥ {resume_id}: {str(e)}")

async def consume():
    consumer = AIOKafkaConsumer(
        'resume_tasks',
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        group_id="resume_rag_worker_group",
        auto_offset_reset='earliest'
    )
    await consumer.start()
    logger.info("ğŸ“¡ Worker å·²ä¸Šçº¿ï¼Œæ­£åœ¨ç›‘å¬ä»»åŠ¡...")

    try:
        async for msg in consumer:
            data = json.loads(msg.value.decode('utf-8'))
            resume_id = data.get("resume_id")
            file_path = data.get("file_path")
            
            await process_resume_rag(resume_id, file_path)
    finally:
        await consumer.stop()

if __name__ == "__main__":
    asyncio.run(consume())